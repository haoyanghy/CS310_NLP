{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 1: Basic Text Processing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of lines:  4689\n",
      "# of characters:  385018\n"
     ]
    }
   ],
   "source": [
    "with open(\"三体3死神永生-刘慈欣.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "print('# of lines: ', len(raw))\n",
    "raw = ''.join(raw) # concatenate all lines into one string\n",
    "print('# of characters: ', len(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T0. Cleaning the raw data\n",
    "\n",
    "1. Replace the special token `\\u3000` with empty string \"\".\n",
    "2. Replace consecutive newlines with just a single one.\n",
    "3. Other cleaning work you can think of.\n",
    "\n",
    "*Hint*: Use `re.sub()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of characters after cleaning: 375711\n",
      "# of characters after cleaning: 375677\n",
      "# of characters after cleaning: 375580\n",
      "# of characters after cleaning: 375580\n"
     ]
    }
   ],
   "source": [
    "cleaned = re.sub('\\u3000', '', raw)\n",
    "print('# of characters after cleaning:', len(cleaned))\n",
    "\n",
    "cleaned = re.sub(r'\\n+', '\\n', cleaned) \n",
    "print('# of characters after cleaning:', len(cleaned))\n",
    "\n",
    "cleaned = re.sub(r' +', ' ', cleaned) # replace multiple spaces with one space\n",
    "print('# of characters after cleaning:', len(cleaned))\n",
    "\n",
    "cleaned = cleaned.strip() # remove leading and trailing spaces\n",
    "print('# of characters after cleaning:', len(cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Count the number of Chinese tokens\n",
    "\n",
    "*Hint*: Use `re.findall()` and the range of Chinese characters in Unicode, i.e., `[\\u4e00-\\u9fa5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chinese tokens: 329946\n"
     ]
    }
   ],
   "source": [
    "num = re.findall(r'[\\u4e00-\\u9fa5]', cleaned)\n",
    "print('Number of Chinese tokens:', len(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Build the vocabulary for all Chinese tokens\n",
    "\n",
    "Use a Python `dict` object or instance of  `collections.Counter()` to count the frequency of each Chinese token.\n",
    "\n",
    "*Hint*: Go through the `raw` string and for each unique Chinese token, add it to the `dict` or `Counter` object with a count of 1. If the token is already in the `dict` or `Counter` object, increment its count by 1.\n",
    "\n",
    "Check the vocabulary size and print the top 20 most frequent Chinese tokens and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3027\n",
      "Top 20 most frequent Chinese tokens:\n",
      "的: 15990\n",
      "一: 6749\n",
      "是: 4837\n",
      "在: 4748\n",
      "了: 4149\n",
      "有: 3656\n",
      "这: 3532\n",
      "个: 3458\n",
      "不: 3117\n",
      "人: 2988\n",
      "中: 2649\n",
      "到: 2632\n",
      "他: 2354\n",
      "上: 2194\n",
      "们: 2164\n",
      "时: 2076\n",
      "心: 2007\n",
      "地: 1953\n",
      "大: 1938\n",
      "来: 1855\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "vocab = collections.Counter(num)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "print('Top 20 most frequent Chinese tokens:')\n",
    "for token, count in vocab.most_common(20):\n",
    "    print(f'{token}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Sentence segmentation\n",
    "\n",
    "Estimate the number of sentences in the `raw` string by separating the sentences with the delimiter punctuations, such as  `。`, `？`, `！` etc.\n",
    "\n",
    "*Hint*: Use `re.split()` and the correct regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  9611\n"
     ]
    }
   ],
   "source": [
    "sentences = re.split(r'[。！？]', raw)\n",
    "sentences = [s.strip() for s in sentences if s.strip()]\n",
    "print(\"Number of sentences: \",len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences obtained with `re.split()` do not contain the delimiter punctuations. What if we want to keep the delimiter punctuations in the sentences?\n",
    "\n",
    "*Hint*: Use `re.findall()` and the correct regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  9615\n"
     ]
    }
   ],
   "source": [
    "sentences = re.findall(r'.+?[。！？]', raw)\n",
    "print(\"Number of sentences: \",len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Count consecutive English and number tokens\n",
    "\n",
    "Estimate the number of consecutive English and number tokens in the `raw` string. Build a vocabulary for them and count their frequency.\n",
    "\n",
    "*Hint*: Use `re.findall()` and the correct regular expression. Use similar method as in T2 to build the vocabulary and count the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 171\n",
      "Top 20 most frequent English and number tokens:\n",
      "AA: 338\n",
      "A: 69\n",
      "I: 66\n",
      "1: 47\n",
      "PIA: 45\n",
      "PDC: 35\n",
      "Ice: 34\n",
      "3: 30\n",
      "IDC: 28\n",
      "DX: 27\n",
      "3906: 27\n",
      "5: 27\n",
      "0: 22\n",
      "G: 20\n",
      "Way: 20\n",
      "647: 19\n",
      "7: 19\n",
      "16: 14\n",
      "4: 13\n",
      "2: 13\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall(r'[a-zA-Z]+|\\d+', raw)\n",
    "\n",
    "vocab = collections.Counter(tokens)\n",
    "\n",
    "print('Vocabulary size:', len(vocab))\n",
    "\n",
    "print('Top 20 most frequent English and number tokens:')\n",
    "for token, count in vocab.most_common(20):\n",
    "    print(f'{token}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5. Mix of patterns\n",
    "\n",
    "There are two characters whose names are \"艾AA\" and \"程心\". Find all sentences where \"艾AA\" and \"程心\" appear together. Consider fullnames only, that is, \"艾AA\" but not \"AA\" alone. \n",
    "\n",
    "*Hint*: You may find the lookbehind or lookahead pattern useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences where \"艾AA\" and \"程心\" appear together:\n",
      "1. 在程心眼中，艾AA是个像鸟一般轻灵的女孩子，充满生机地围着她飞来飞去。\n",
      "2. 程心听到有人叫自己的名字，转身一看，竟是艾AA正向这里跑过来。\n",
      "3. 程心让艾AA在原地等着自己，但AA坚持要随程心去，只好让她上了车。\n",
      "4. 程心和艾AA是随最早的一批移民来到澳大利亚的。\n",
      "5. #第三部\n",
      "　　【广播纪元7年，程心】\n",
      "　　艾AA说程心的眼睛比以前更明亮更美丽了，也许她没有说谎。\n",
      "6. ”坐在程心旁边的艾AA大叫起来，引来众人不满的侧目。\n",
      "7. 这天，艾AA来找程心。\n",
      "8. 是艾AA建议程心报名参加试验的，她认为这是为星环公司参与掩体工程而树立公众形象的一次极佳的免费广告，同时，她和程心都清楚试验是经过严密策划的，只是看上去刺激，基本没什么危险。\n",
      "9. 在返回的途中，当太空艇与地球的距离缩小到三十万千米以内、通信基本没有延时时，程心给艾AA打电话，告诉了她与维德会面的事。\n",
      "10. 与此同时，程心和艾AA进入冬眠。\n",
      "11. 程心到亚洲一号的冬眠中心唤醒了冬眠中的艾AA，两人回到了地球。\n",
      "12. 程心现在身处的世界是一个白色的球形空间，她看到艾AA飘浮在附近，和她一样身穿冬眠时的紧身服，头发湿漉漉的，四肢无力地摊开，显然也是刚刚醒来。\n",
      "13. 对此程心感到很欣慰，到了新世界后，艾AA应该有一个美好的新生活了。\n",
      "14. 程心想到了云天明和艾AA，他们在地面上，应该是安全的，但现在双方已经无法联系，她甚至都没能和他说上一句话。\n",
      "15. 程心和关一帆再次拥抱在一起，他们都为艾AA和云天明流下了欣慰的泪水，幸福地感受着那两个人在十八万个世纪前的幸福，在这种幸福中，他们绝望的心灵变得无比宁静了。\n",
      "16. ”\n",
      "　　智子的话让程心想到了云天明和艾AA刻在岩石上的字，但关一帆想到的更多，他注意到了智子提到的一个词：田园时代。\n"
     ]
    }
   ],
   "source": [
    "pattern = r'[^。！？]*艾AA[^。！？]*程心[^。！？]*[。！？]|[^。！？]*程心[^。！？]*艾AA[^。！？]*[。！？]'\n",
    "\n",
    "matching_sentences = re.findall(pattern, raw)\n",
    "\n",
    "for i, sentence in enumerate(matching_sentences, 1):\n",
    "    print(f'{i}. {sentence.strip()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
