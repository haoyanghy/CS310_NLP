{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 14: In-Context Learning and Prompting\n",
    "\n",
    "In this lab, we will practice some in-context learning techniques, such as few-shot learning and chain-of-thought prompting, for solving QA problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Run LLMs locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Install llama.cpp\n",
    "\n",
    "Build the [llama.cpp](https://github.com/ggml-org/llama.cpp) tool, or download the binaries from the [release page](https://github.com/ggml-org/llama.cpp/releases).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Download model\n",
    "\n",
    "We are going to download the model that is quantized and format-converted to `gguf` format.\n",
    "\n",
    "**Model option a**: \n",
    "- Using the `huggingface-cli` tool.\n",
    "- Following the tutorial here: (Qwen2.5-7B-Instruct-GGUF)[https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF]\n",
    "\n",
    "A quick command to download the model is:\n",
    "```bash\n",
    "huggingface-cli download Qwen/Qwen1.5-7B-Chat-GGUF qwen1_5-7b-chat-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "\n",
    "**Model option b**: \n",
    "- Or you can download the ChatGLM-3 model from ModelScope: https://modelscope.cn/models/ZhipuAI/chatglm3-6b/files\n",
    "  - `model.safetensors.index.json`, `config.json`, `configuration.json`\n",
    "  - `model-00001-of-00007.safetensors` to `model-00007-of-00007.safetensors`\n",
    "  - `tokenizer_config.json`, `tokenizer.model`\n",
    "Put all the files in a folder such as `./chatglm3-6b`. \n",
    "- Then use tools like [`chatglm.cpp`](https://github.com/li-plus/chatglm.cpp) to manually convert the model weights to `ggml` format.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Run model\n",
    "\n",
    "You can run the model with following command:\n",
    "\n",
    "```bash\n",
    "llama-cli -m $MODEL_PATH\n",
    "```\n",
    "\n",
    "Then you can start interacting with the model in command line. Try to solve the following problems.\n",
    " - Use zero-shot and few-shot prompting to solve the problems.\n",
    " - Add Chain-of-Thought prompt if needed.\n",
    "\n",
    "\n",
    "Try solving these problems with prompting:\n",
    "1. Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there? A: \n",
    "2. 鸡和兔在一个笼子里，共有35个头，94只脚，那么鸡有多少只，兔有多少只？\n",
    "3. Q: 242342 + 423443 = ? A: \n",
    "4. 一个人花8块钱买了一只鸡，9块钱卖掉了，然后他觉得不划算，花10块钱又买回来了，11块卖给另外一个人。问他赚了多少?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Practice few-shot prompting\n",
    "\n",
    "For this pratice, you need to first download the [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) model from HuggingFace, by running the following command:\n",
    "\n",
    "```bash\n",
    "huggingface-cli download Qwen/Qwen2.5-7B --local-dir $MODEL_PATH\n",
    "```\n",
    "\n",
    "The task set we use is [MMLU](https://huggingface.co/datasets/cais/mmlu). You need to download the zip file and extract it to the `./MMLU` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define some helper functions for constructing prompts and running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "def format_example(input_list):\n",
    "    prompt = input_list[0]\n",
    "    k = len(input_list) - 2\n",
    "    for j in range(k):\n",
    "        prompt += \"\\n{}. {}\".format(choices[j], input_list[j+1])\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "def format_shots(prompt_data):\n",
    "    prompt = \"\"\n",
    "    for data in prompt_data:\n",
    "        prompt += data[0]\n",
    "        k = len(data) - 2\n",
    "        for j in range(k):\n",
    "            prompt += \"\\n{}. {}\".format(choices[j], data[j+1])\n",
    "        prompt += \"\\nAnswer:\"\n",
    "        prompt += data[k+1] + \"\\n\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(input_list, subject, prompt_data):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
    "        format_subject(subject)\n",
    "    )\n",
    "    prompt += format_shots(prompt_data)\n",
    "    prompt += format_example(input_list)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `inference()` function constructs the full input by prepending the few-shot examples to the `input_text`, and generate **1** token as the output, because the task modality is multiple choice question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(tokenizer, model, input_text, subject, prompt_data):\n",
    "    if len(prompt_data) > 0:\n",
    "        full_input = gen_prompt(input_text, subject, prompt_data) # add few-shot examples\n",
    "    else:\n",
    "        full_input = input_text\n",
    "    inputs = tokenizer(full_input, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    ids = inputs['input_ids']\n",
    "    outputs = model.generate(\n",
    "                ids,\n",
    "                attention_mask = inputs['attention_mask'],\n",
    "                pad_token_id = tokenizer.eos_token_id,\n",
    "                max_new_tokens = 1, # Generate one token because it is multiple choice question\n",
    "                output_scores = True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "    logits = outputs['scores'][0][0]    #The first token\n",
    "    probs = (\n",
    "            torch.nn.functional.softmax(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        logits[tokenizer(\"A\").input_ids[0]],\n",
    "                        logits[tokenizer(\"B\").input_ids[0]],\n",
    "                        logits[tokenizer(\"C\").input_ids[0]],\n",
    "                        logits[tokenizer(\"D\").input_ids[0]],\n",
    "                    ]\n",
    "                ),\n",
    "                dim=0,\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "    )\n",
    "    output_text = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[np.argmax(probs)]\n",
    "    conf = np.max(probs)\n",
    "        \n",
    "    return output_text, full_input, conf.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model_path = '../lab13/qwen'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          use_fast=True,\n",
    "                                          unk_token=\"<unk>\",\n",
    "                                          bos_token=\"<s>\", eos_token=\"</s>\",\n",
    "                                          add_bos_token=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the json data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "prompt = {}\n",
    "\n",
    "with open(f\"./MMLU/MMLU_ID_test.json\",'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open(f\"./MMLU/MMLU_ID_prompt.json\",'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data is organized by subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics'])\n",
      "\n",
      "['At breakfast, lunch, and dinner, Joe randomly chooses with equal '\n",
      " 'probabilities either an apple, an orange, or a banana to eat. On a given '\n",
      " 'day, what is the probability that Joe will eat at least two different kinds '\n",
      " 'of fruit?',\n",
      " '\\\\frac{7}{9}',\n",
      " '\\\\frac{8}{9}',\n",
      " '\\\\frac{5}{9}',\n",
      " '\\\\frac{9}{11}',\n",
      " 'B']\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())\n",
    "\n",
    "print()\n",
    "pprint(data['high_school_mathematics'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompts also come in subjects, and each subject has a list of 5 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(prompt['high_school_mathematics']))\n",
    "print(len(prompt['high_school_physics']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stick to one subject, `high_school_mathematics` for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'high_school_mathematics'\n",
    "data_sub = data[subject]\n",
    "prompt_sub = prompt[subject]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take one input example and generate the full prompt by calling `gen_prompt()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice questions (with answers) about  high school mathematics.\n",
      "\n",
      "Joe was in charge of lights for a dance. The red light blinks every two seconds, the yellow light every three seconds, and the blue light every five seconds. If we include the very beginning and very end of the dance, how many times during a seven minute dance will all the lights come on at the same time? (Assume that all three lights blink simultaneously at the very beginning of the dance.)\n",
      "A. 3\n",
      "B. 15\n",
      "C. 6\n",
      "D. 5\n",
      "Answer:B\n",
      "\n",
      "Five thousand dollars compounded annually at an $x\\%$ interest rate takes six years to double. At the same interest rate, how many years will it take $\\$300$ to grow to $\\$9600$?\n",
      "A. 12\n",
      "B. 1\n",
      "C. 30\n",
      "D. 5\n",
      "Answer:C\n",
      "\n",
      "The variable $x$ varies directly as the square of $y$, and $y$ varies directly as the cube of $z$. If $x$ equals $-16$ when $z$ equals 2, what is the value of $x$ when $z$ equals $\\frac{1}{2}$?\n",
      "A. -1\n",
      "B. 16\n",
      "C. -\\frac{1}{256}\n",
      "D. \\frac{1}{16}\n",
      "Answer:C\n",
      "\n",
      "Simplify and write the result with a rational denominator: $$\\sqrt{\\sqrt[3]{\\sqrt{\\frac{1}{729}}}}$$\n",
      "A. \\frac{3\\sqrt{3}}{3}\n",
      "B. \\frac{1}{3}\n",
      "C. \\sqrt{3}\n",
      "D. \\frac{\\sqrt{3}}{3}\n",
      "Answer:D\n",
      "\n",
      "Ten students take a biology test and receive the following scores: 45, 55, 50, 70, 65, 80, 40, 90, 70, 85. What is the mean of the students’ test scores?\n",
      "A. 55\n",
      "B. 60\n",
      "C. 62\n",
      "D. 65\n",
      "Answer:D\n",
      "\n",
      "At breakfast, lunch, and dinner, Joe randomly chooses with equal probabilities either an apple, an orange, or a banana to eat. On a given day, what is the probability that Joe will eat at least two different kinds of fruit?\n",
      "A. \\frac{7}{9}\n",
      "B. \\frac{8}{9}\n",
      "C. \\frac{5}{9}\n",
      "D. \\frac{9}{11}\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "input_text = data_sub[3]\n",
    "prompt_text = gen_prompt(input_text, subject, prompt_sub)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaron\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "output, _, conf = inference(tokenizer, model, input_text, subject, prompt_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "0.7166428565979004\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with zero-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_prompt = '''\n",
    "    At breakfast, lunch, and dinner, Joe randomly chooses with equal probabilities either an apple, an orange, or a banana to eat. On a given day, what is the probability that Joe will eat at least two different kinds of fruit?\n",
    "    A. \\frac{7}{9}\n",
    "    B. \\frac{8}{9}\n",
    "    C. \\frac{5}{9}\n",
    "    D. \\frac{9}{11}\n",
    "    Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "0.49263322353363037\n"
     ]
    }
   ],
   "source": [
    "output, _, conf = inference(tokenizer, model, zs_prompt, subject, prompt_data=[])\n",
    "print(output)\n",
    "print(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
