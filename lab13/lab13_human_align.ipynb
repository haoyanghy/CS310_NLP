{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b29f68",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 13: Human Alignment\n",
    "\n",
    "In this lab, we will practice two tasks:\n",
    "- Using the code framework for training a reward model that assigns scores to pairs of sentences. \n",
    "- Getting familiar with the code framework for Direct Preference Optimization (DPO).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "149117e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2573d",
   "metadata": {},
   "source": [
    "## T1. Defining Reward Model\n",
    "\n",
    "\n",
    "We will use the [LlamaForCausalLM](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaForCausalLM) model from HuggingFace, as the basis for our reward model.\n",
    "\n",
    "First, two internal forward functions are to be implemented:\n",
    "- `_forward_rm`: it takes the input ids and attention masks of a sequence (user input + response), and returns the reward scores.\n",
    "  - The reward scores are in tensor of same shape as the input ids, with **one reward score for each token**.\n",
    "  - Reward scores are calculated by calling a linear layer `self.reward_head` on the last hidden state (of the entire sequence).\n",
    "- `_forward_lmloss`: it takes the input of same format, but returns the regular language modeling loss.\n",
    "  - Logits are computed by calling `self.lm_head` on the last hidden state.\n",
    "  - The `response_ids` are used as the target for the `nn.CrossEntropyLoss()`.\n",
    "\n",
    "Then, define the `forward` function, which takes the input ids and attention masks of two sequences, and returns the combined loss.\n",
    "- Compute `reward1` on the first sequence (positve example) and `reward2` on the second sequence (negative example).\n",
    "- Calculate their difference in `logits`\n",
    "- Reward loss is computed by calling `F.binary_cross_entropy_with_logits(logits, label)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14d6d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRewardModel(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # A linear layer to map hidden states to a scalar, as the final reward\n",
    "        self.reward_head = nn.Linear(config.hidden_size, 1, bias=False)\n",
    "\n",
    "    def _forward_rm(self, input_ids, attention_mask, **kargs):\n",
    "        \"\"\"\n",
    "        input_ids: input token ids\n",
    "        attention_mask: attention mask\n",
    "        Return: reward scores, output from self.reward_head\n",
    "        \"\"\"\n",
    "        # Call self.model.forward()  to get the hidden states\n",
    "        output = self.model.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, \n",
    "            return_dict=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "        ### START YOUR CODE ###\n",
    "        # Feed the last hidden state from output to self.reward_head to get the reward score\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        rewards = self.reward_head(last_hidden_state)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return rewards \n",
    "    \n",
    "    def _forward_lmloss(self, prompt_ids, lm_attn_mask, response_ids):\n",
    "        \"\"\"\n",
    "        input_ids: input token ids\n",
    "        attention_mask: attention mask\n",
    "        Return: cross-entropy loss for language modeling\n",
    "        \"\"\" \n",
    "        # Call self.model.forward()  to get the hidden states\n",
    "        outputs = self.model.forward(\n",
    "            input_ids=prompt_ids,\n",
    "            attention_mask=lm_attn_mask,\n",
    "            return_dict=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "         # 获取最后一层的隐藏状态\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        # 通过lm_head得到logits\n",
    "        logits = self.lm_head(last_hidden_state)\n",
    "        \n",
    "        # 计算交叉熵损失\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # 将logits和response_ids调整为正确的形状\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        response_ids = response_ids.view(-1)\n",
    "        loss = criterion(logits, response_ids)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def forward(self, sent1_idx, attention_mask_1, sent2_idx, attention_mask_2, labels, prompt_ids, lm_attn_mask, response_ids, **kargs):\n",
    "        \"\"\"\n",
    "        sent1_idx: User input ids + positive output ids\n",
    "        attention_mask_1: Attention mask for sent1_idx\n",
    "        sent2_idx: User input ids + negative output ids\n",
    "        attention_mask_2: Attention mask for sent2_idx\n",
    "\n",
    "        labels: Positive output ids (all zeros)\n",
    "\n",
    "        prompt_ids: User input ids + positive output ids\n",
    "        lm_attn_mask: Attention mask for prompt_ids\n",
    "        response_ids: Target ids for calculating cross-entropy loss\n",
    "        \"\"\"\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        # Reward for positive example\n",
    "        reward0 = self._forward_rm(sent1_idx, attention_mask_1)\n",
    "        # Reward for negative example\n",
    "        reward1 = self._forward_rm(sent2_idx, attention_mask_2)\n",
    "        # Calculate the reward difference\n",
    "        logits = reward0 - reward1  # Shape: [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Squeeze the last dimension to match labels shape\n",
    "        logits = logits.squeeze(-1)  # Shape: [batch_size, seq_len]\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        # Compute the reward modeling loss\n",
    "        rm_loss = F.binary_cross_entropy_with_logits(logits, labels.to(logits.dtype), reduction=\"mean\")\n",
    "\n",
    "        # Compute the language modeling loss \n",
    "        lm_loss = self._forward_lmloss(prompt_ids, lm_attn_mask, response_ids)\n",
    "\n",
    "        # Final loss\n",
    "        loss = rm_loss + lm_loss\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ffbd17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type qwen2 to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of LlamaRewardModel were not initialized from the model checkpoint at ./qwen and are newly initialized: ['reward_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "#model = LlamaRewardModel.from_pretrained('/Users/xy/models/llama-2-7b-hf')\n",
    "model = LlamaRewardModel.from_pretrained('./qwen')\n",
    "# model = LlamaRewardModel.from_pretrained(\n",
    "#     \"Qwen/Qwen-7B\", \n",
    "#     revision=\"main\",  \n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# You expect to see the model correctly initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f020bf",
   "metadata": {},
   "source": [
    "## T2. Load Preference Data\n",
    "\n",
    "We will load the preference dataset from `Anthropic/hh-rlhf` for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfb3638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer\n",
    "from transformers.hf_argparser import HfArg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "242a70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arguments(TrainingArguments):\n",
    "    model_name_or_path: str = HfArg(\n",
    "        default=\"./qwen\", # The path to your model\n",
    "        help=\"The model name or path\"\n",
    "    )\n",
    "    \n",
    "    # Preference dataset\n",
    "    data_path: str = HfArg(\n",
    "        default='./hh-rlhf', # The path to the preference dataset\n",
    "        help=\"The path of preference dataset, e.g., `Anthropic/hh-rlhf`\",\n",
    "    )\n",
    "\n",
    "    model_max_length: int = HfArg(default=512, help=\"Maximum sequence length.\")\n",
    "\n",
    "    bf16: bool = HfArg(\n",
    "        default=True,\n",
    "        help=\"Whether to use bf16 (mixed) precision instead of 32-bit.\",\n",
    "    )\n",
    "\n",
    "    # Hyper-parameters for DPO loss\n",
    "    beta: float = HfArg(\n",
    "        default=0.1,\n",
    "        help=\"The beta factor in DPO loss.\"\n",
    "        \"Higher beta means less divergence from the initial policy.\",\n",
    "    )\n",
    "\n",
    "    output_dir: str = HfArg(\n",
    "        default=\"output\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ad62a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./qwen\n",
      "./hh-rlhf\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "args = Arguments()\n",
    "print(args.model_name_or_path)\n",
    "print(args.data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7db53",
   "metadata": {},
   "source": [
    "The following function prepares the preference dataset in a user-friendly view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ace96a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(split, data_path):\n",
    "    dataset = load_dataset(split=split, path=data_path)\n",
    "\n",
    "    def split_prompt_and_responses_hh(sample):\n",
    "        search_term = \"\\n\\nAssistant:\"\n",
    "        search_term_idx = sample[\"chosen\"].rfind(search_term)\n",
    "        assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "        prompt = sample[\"chosen\"][:search_term_idx + len(search_term)]\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"][len(prompt):],\n",
    "            \"rejected\": sample[\"rejected\"][len(prompt):],\n",
    "        }\n",
    "\n",
    "    return dataset.map(split_prompt_and_responses_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c5c85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      " I haven't even thought about it.\n",
      " Ass.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "train_dataset = get_data('train', args.data_path)\n",
    "\n",
    "prompt = train_dataset[0]['prompt']\n",
    "chosen = train_dataset[0]['chosen']\n",
    "rejected = train_dataset[0]['rejected']\n",
    "print(prompt[:45])\n",
    "print(chosen)\n",
    "print(rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493a453",
   "metadata": {},
   "source": [
    "Now, load tokenizer and tokenize some sample data.\n",
    "\n",
    "- `sent1_encoded` is the tokenized result of `prompt + chosen` (positive example)\n",
    "- `sent2_encoded` is the tokenized result of `prompt + rejected` (negative example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "61188b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=False)\n",
    "\n",
    "\n",
    "### START YOUR CODE ###\n",
    "sent1_encoded = tokenizer(\n",
    "    prompt + chosen,\n",
    "    truncation=True,\n",
    "    max_length=args.model_max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "sent2_encoded = tokenizer(\n",
    "    prompt + rejected,\n",
    "    truncation=True,\n",
    "    max_length=args.model_max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad4b95",
   "metadata": {},
   "source": [
    "Pad two sequences (input ids and attention masks) to same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "875322d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 185])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 185])\n"
     ]
    }
   ],
   "source": [
    "sent1_idx = sent1_encoded['input_ids']\n",
    "sent2_idx = sent2_encoded['input_ids']\n",
    "\n",
    "# Pad input ids\n",
    "max_len = max(sent1_idx.shape[1], sent2_idx.shape[1])\n",
    "sent1_idx = torch.nn.functional.pad(sent1_idx, (0, max_len - sent1_idx.shape[1]), value=tokenizer.pad_token_id)\n",
    "sent2_idx = torch.nn.functional.pad(sent2_idx, (0, max_len - sent2_idx.shape[1]), value=tokenizer.pad_token_id)\n",
    "\n",
    "# Pad attention masks\n",
    "sent1_attn_mask = sent1_encoded['attention_mask']\n",
    "sent2_attn_mask = sent2_encoded['attention_mask']\n",
    "sent1_attn_mask = torch.nn.functional.pad(sent1_attn_mask, (0, max_len - sent1_attn_mask.shape[1]), value=0)\n",
    "sent2_attn_mask = torch.nn.functional.pad(sent2_attn_mask, (0, max_len - sent2_attn_mask.shape[1]), value=0)\n",
    "\n",
    "print(sent1_idx.shape)\n",
    "print(sent2_idx.shape)\n",
    "print(sent1_attn_mask.shape)\n",
    "print(sent2_attn_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b47f16",
   "metadata": {},
   "source": [
    "Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "87d90e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'sent1_idx': sent1_idx, \n",
    "    'attention_mask_1': sent1_attn_mask, \n",
    "    'sent2_idx': sent2_idx, \n",
    "    'attention_mask_2': sent2_attn_mask, \n",
    "\n",
    "    'labels': torch.zeros_like(sent1_idx), \n",
    "\n",
    "    'prompt_ids': sent1_encoded['input_ids'], \n",
    "    'lm_attn_mask': sent1_encoded['attention_mask'], \n",
    "    'response_ids': sent1_encoded['input_ids'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "790ff1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.3644)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**input_data)\n",
    "    print(output)\n",
    "\n",
    "# You expect to see a single loss value\n",
    "# Runtime Error is likely to because by the implementation of the internal forward functions\n",
    "# You can use the following code to help you debug\n",
    "# r1 = model._forward_rmloss(sent1_idx, sent1_attn_mask)\n",
    "# print(r1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fb68c",
   "metadata": {},
   "source": [
    "## T3. (Optional) DPO Training\n",
    "\n",
    "You need to install the [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/en/index) library first.\n",
    "\n",
    "```bash\n",
    "pip install trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c534fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, HfArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eed20643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Parse arguments\n",
    "    parser = HfArgumentParser(Arguments)\n",
    "    args = parser.parse_args_into_dataclasses()[0]\n",
    "    \n",
    "    # Load policy model\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    # Load reference model\n",
    "    model_ref = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    # Freeze reference model\n",
    "    model_ref.eval()\n",
    "    for param in model_ref.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Tokenizer and data\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        model_max_length=args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True,\n",
    "    )\n",
    "    train_dataset = get_data(\"train\", args.data_path)\n",
    "\n",
    "    # Training arguments\n",
    "    kwargs = dict(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    dpo_trainer = DPOTrainer(**kwargs)\n",
    "    dpo_trainer.train()\n",
    "    dpo_trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "21e2a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output_dir OUTPUT_DIR]\n",
      "                             [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                             [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                             [--do_predict [DO_PREDICT]]\n",
      "                             [--eval_strategy {no,steps,epoch}]\n",
      "                             [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                             [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                             [--eval_delay EVAL_DELAY]\n",
      "                             [--torch_empty_cache_steps TORCH_EMPTY_CACHE_STEPS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,warmup_stable_decay}]\n",
      "                             [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]\n",
      "                             [--warmup_ratio WARMUP_RATIO]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--log_level {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "                             [--no_log_on_each_node]\n",
      "                             [--logging_dir LOGGING_DIR]\n",
      "                             [--logging_strategy {no,steps,epoch}]\n",
      "                             [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "                             [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
      "                             [--no_logging_nan_inf_filter]\n",
      "                             [--save_strategy {no,steps,epoch,best}]\n",
      "                             [--save_steps SAVE_STEPS]\n",
      "                             [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                             [--save_safetensors [SAVE_SAFETENSORS]]\n",
      "                             [--no_save_safetensors]\n",
      "                             [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "                             [--save_only_model [SAVE_ONLY_MODEL]]\n",
      "                             [--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]]\n",
      "                             [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]\n",
      "                             [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n",
      "                             [--data_seed DATA_SEED]\n",
      "                             [--jit_mode_eval [JIT_MODE_EVAL]]\n",
      "                             [--use_ipex [USE_IPEX]] [--bf16 [BF16]]\n",
      "                             [--no_bf16] [--fp16 [FP16]]\n",
      "                             [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                             [--half_precision_backend {auto,apex,cpu_amp}]\n",
      "                             [--bf16_full_eval [BF16_FULL_EVAL]]\n",
      "                             [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl,mccl}]\n",
      "                             [--tpu_num_cores TPU_NUM_CORES]\n",
      "                             [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
      "                             [--debug DEBUG [DEBUG ...]]\n",
      "                             [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                             [--eval_steps EVAL_STEPS]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]\n",
      "                             [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                             [--disable_tqdm DISABLE_TQDM]\n",
      "                             [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                             [--no_remove_unused_columns]\n",
      "                             [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                             [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                             [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                             [--greater_is_better GREATER_IS_BETTER]\n",
      "                             [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
      "                             [--fsdp FSDP]\n",
      "                             [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
      "                             [--fsdp_config FSDP_CONFIG] [--tp_size TP_SIZE]\n",
      "                             [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "                             [--accelerator_config ACCELERATOR_CONFIG]\n",
      "                             [--deepspeed DEEPSPEED]\n",
      "                             [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                             [--optim {adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,adamw_torch_4bit,adamw_torch_8bit,ademamix,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,ademamix_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_ademamix_32bit,paged_ademamix_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop,rmsprop_bnb,rmsprop_bnb_8bit,rmsprop_bnb_32bit,galore_adamw,galore_adamw_8bit,galore_adafactor,galore_adamw_layerwise,galore_adamw_8bit_layerwise,galore_adafactor_layerwise,lomo,adalomo,grokadamw,schedule_free_radam,schedule_free_adamw,schedule_free_sgd,apollo_adamw,apollo_adamw_layerwise}]\n",
      "                             [--optim_args OPTIM_ARGS]\n",
      "                             [--adafactor [ADAFACTOR]]\n",
      "                             [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                             [--length_column_name LENGTH_COLUMN_NAME]\n",
      "                             [--report_to REPORT_TO]\n",
      "                             [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                             [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "                             [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
      "                             [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "                             [--no_dataloader_pin_memory]\n",
      "                             [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]\n",
      "                             [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "                             [--no_skip_memory_metrics]\n",
      "                             [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
      "                             [--push_to_hub [PUSH_TO_HUB]]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--hub_private_repo HUB_PRIVATE_REPO]\n",
      "                             [--hub_always_push [HUB_ALWAYS_PUSH]]\n",
      "                             [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "                             [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n",
      "                             [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
      "                             [--include_for_metrics INCLUDE_FOR_METRICS [INCLUDE_FOR_METRICS ...]]\n",
      "                             [--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]\n",
      "                             [--no_eval_do_concat_batches]\n",
      "                             [--fp16_backend {auto,apex,cpu_amp}]\n",
      "                             [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
      "                             [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "                             [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
      "                             [--mp_parameters MP_PARAMETERS]\n",
      "                             [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "                             [--full_determinism [FULL_DETERMINISM]]\n",
      "                             [--torchdynamo TORCHDYNAMO]\n",
      "                             [--ray_scope RAY_SCOPE]\n",
      "                             [--ddp_timeout DDP_TIMEOUT]\n",
      "                             [--torch_compile [TORCH_COMPILE]]\n",
      "                             [--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
      "                             [--torch_compile_mode TORCH_COMPILE_MODE]\n",
      "                             [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n",
      "                             [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n",
      "                             [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n",
      "                             [--optim_target_modules OPTIM_TARGET_MODULES]\n",
      "                             [--batch_eval_metrics [BATCH_EVAL_METRICS]]\n",
      "                             [--eval_on_start [EVAL_ON_START]]\n",
      "                             [--use_liger_kernel [USE_LIGER_KERNEL]]\n",
      "                             [--eval_use_gather_object [EVAL_USE_GATHER_OBJECT]]\n",
      "                             [--average_tokens_across_devices [AVERAGE_TOKENS_ACROSS_DEVICES]]\n",
      "                             [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                             [--data_path DATA_PATH]\n",
      "                             [--model_max_length MODEL_MAX_LENGTH]\n",
      "                             [--beta BETA]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=c:\\Users\\aaron\\AppData\\Roaming\\jupyter\\runtime\\kernel-v39125676c60ab48ffe48babe09c211b29f92c6dde.json could match --fp16, --fp16_opt_level, --fp16-opt-level, --fp16_full_eval, --fp16-full-eval, --fsdp, --fsdp_min_num_params, --fsdp-min-num-params, --fsdp_config, --fsdp-config, --fsdp_transformer_layer_cls_to_wrap, --fsdp-transformer-layer-cls-to-wrap, --fp16_backend, --fp16-backend, --full_determinism, --full-determinism\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaron\\anaconda3\\envs\\py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
