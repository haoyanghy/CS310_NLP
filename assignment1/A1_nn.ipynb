{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 1. Neural Text Classification\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from Lab 2. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "import jieba\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': '卖油条小刘说：我说', 'choices': ['0', '1'], 'label': [0], 'id': 'train_0'}\n",
      "{'sentence': '保姆小张说：干啥子嘛？', 'choices': ['0', '1'], 'label': [0], 'id': 'train_1'}\n",
      "{'sentence': '卖油条小刘说：你看你往星空看月朦胧，鸟朦胧', 'choices': ['0', '1'], 'label': [1], 'id': 'train_2'}\n",
      "{'sentence': '卖油条小刘说：咱是不是歇一下这双，疲惫的双腿？', 'choices': ['0', '1'], 'label': [0], 'id': 'train_3'}\n",
      "{'sentence': '卖油条小刘说：快把我累死了', 'choices': ['0', '1'], 'label': [0], 'id': 'train_4'}\n"
     ]
    }
   ],
   "source": [
    "class JSONLinesIterator:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            self.data = [json.loads(line) for line in file]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n",
    "\n",
    "train_iter = iter(JSONLinesIterator('train.jsonl'))\n",
    "\n",
    "for _ in range(5):\n",
    "    print(next(train_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['卖', '油条', '小', '刘说', '：', '我', '说']\n",
      "['保姆', '小张', '说', '：', '干', '啥子', '嘛', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '你', '看', '你', '往', '星空', '看', '月', '朦胧', '，', '鸟', '朦胧']\n",
      "['卖', '油条', '小', '刘说', '：', '咱', '不是', '是不是', '歇', '一下', '这', '双', '，', '疲惫', '的', '双腿', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '快', '把', '我', '累死', '了']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '说', '亲爱', '的', '大姐', '你', '贵姓', '啊', '？']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization \n",
    "def basic_tokenizer(sentence):\n",
    "    if not sentence or not isinstance(sentence, str):\n",
    "        return []\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]', sentence)\n",
    "    return tokens\n",
    "\n",
    "def improved_tokenizer(sentence):\n",
    "    if not sentence or not isinstance(sentence, str):\n",
    "        return []\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]|\\d+|[a-zA-Z]+|[^\\u4e00-\\u9fff\\da-zA-Z\\s]', sentence)\n",
    "    return tokens\n",
    "\n",
    "def jieba_tokenizer(sentence):\n",
    "    if not sentence or not isinstance(sentence, str):\n",
    "        return []\n",
    "    seg_list = jieba.cut_for_search(sentence)\n",
    "    tokens = []\n",
    "    for seg in seg_list:\n",
    "        tokens.append(seg)  # Keep only Chinese characters\n",
    "    return tokens\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield jieba_tokenizer(item['sentence'])\n",
    "\n",
    "# Check the output of yield_tokens()\n",
    "count = 0\n",
    "for tokens in yield_tokens(iter(JSONLinesIterator('train.jsonl'))): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[440, 574, 80, 767, 1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(iter(JSONLinesIterator('train.jsonl'))), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "print(vocab(['卖', '油条', '小', '刘说', '：', '我', '说']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[440, 574, 80, 767, 1, 3, 2]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(jieba_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "tokens = text_pipeline('卖油条小刘说：我说')\n",
    "print(tokens)\n",
    "\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for _item in batch:\n",
    "        _label = _item[\"label\"]\n",
    "        _sentence = _item[\"sentence\"]\n",
    "        label_list.append(label_pipeline(_label[0]))\n",
    "        token_ids = torch.tensor(text_pipeline(_sentence), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)  \n",
    "    token_ids = torch.cat(token_ids_list, dim=0)  \n",
    "    offsets = torch.cumsum(torch.tensor(offsets[:-1]), dim=0)  \n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in this batch:  88\n",
      "Number of examples in one batch:  8\n",
      "Example 0:  tensor([440, 574,  80, 767,   1,   3,   2])\n",
      "Example 7:  tensor([ 440,  574,   80,  767,    1, 2347])\n"
     ]
    }
   ],
   "source": [
    "# Use collate_batch to generate the dataloader\n",
    "train_iter = JSONLinesIterator('train.jsonl')\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "print('Number of tokens in this batch: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 0: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 7: ', token_ids[offsets[7]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_class)\n",
    "        )\n",
    "        # self.init_weights()\n",
    "\n",
    "    # def init_weights(self):\n",
    "    #     initrange = 0.5\n",
    "    #     self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    #     self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "    #     self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        out = self.fc(embedded)\n",
    "        return out\n",
    "    \n",
    "# Build the model\n",
    "train_iter = iter(JSONLinesIterator('train.jsonl'))\n",
    "num_class = len(set([item[\"label\"][0] for item in train_iter]))  # Extract label from JSON data\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64 \n",
    "hidden_dim = 128 \n",
    "model = TextClassificationModel(vocab_size, emsize, hidden_dim, num_class).to(device)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        if i == 0:\n",
    "            break\n",
    "print('output size:', output.size())\n",
    "\n",
    "EPOCHS = 10  \n",
    "LR = 1  \n",
    "BATCH_SIZE = 8  \n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids, offsets) \n",
    "        labels = labels.squeeze()  \n",
    "        try:\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            print('token_ids: ', token_ids)\n",
    "            print('offsets: ', offsets)\n",
    "            raise\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(output, dim=1)  # Get the predicted class (index of max logit) for each example\n",
    "        total_acc += (predicted == labels).sum().item()  # Count correct predictions (True = 1, False = 0)\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    all_labels = []\n",
    "    all_predicted = []\n",
    "\n",
    "    for idx, (labels, text, offsets) in enumerate(dataloader):\n",
    "        output = model(text, offsets) \n",
    "        labels = labels.squeeze()  \n",
    "        loss = criterion(output, labels)\n",
    "        _, predicted = torch.max(output, dim=1) \n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        total_acc += (predicted == labels).sum().item()  # Count correct predictions\n",
    "        total_count += labels.size(0)\n",
    "    accuracy = total_acc / total_count\n",
    "    precision = precision_score(all_labels, all_predicted, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predicted, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, valid, and test data\n",
    "train_iter = JSONLinesIterator('train.jsonl')\n",
    "test_iter = JSONLinesIterator('test.jsonl')\n",
    "# train_dataset = to_map_style_dataset(train_iter)\n",
    "# test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_iter) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_iter, [num_train, len(train_iter) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1506 batches | accuracy    0.711\n",
      "| epoch   1 |  1000/ 1506 batches | accuracy    0.712\n",
      "| epoch   1 |  1500/ 1506 batches | accuracy    0.708\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  9.14s | valid accuracy    0.748 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1506 batches | accuracy    0.720\n",
      "| epoch   2 |  1000/ 1506 batches | accuracy    0.702\n",
      "| epoch   2 |  1500/ 1506 batches | accuracy    0.708\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  8.30s | valid accuracy    0.744 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1506 batches | accuracy    0.713\n",
      "| epoch   3 |  1000/ 1506 batches | accuracy    0.710\n",
      "| epoch   3 |  1500/ 1506 batches | accuracy    0.720\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  7.59s | valid accuracy    0.740 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1506 batches | accuracy    0.714\n",
      "| epoch   4 |  1000/ 1506 batches | accuracy    0.716\n",
      "| epoch   4 |  1500/ 1506 batches | accuracy    0.713\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  7.19s | valid accuracy    0.743 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1506 batches | accuracy    0.720\n",
      "| epoch   5 |  1000/ 1506 batches | accuracy    0.719\n",
      "| epoch   5 |  1500/ 1506 batches | accuracy    0.705\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  7.24s | valid accuracy    0.743 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1506 batches | accuracy    0.707\n",
      "| epoch   6 |  1000/ 1506 batches | accuracy    0.711\n",
      "| epoch   6 |  1500/ 1506 batches | accuracy    0.726\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  7.15s | valid accuracy    0.743 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1506 batches | accuracy    0.713\n",
      "| epoch   7 |  1000/ 1506 batches | accuracy    0.719\n",
      "| epoch   7 |  1500/ 1506 batches | accuracy    0.712\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  7.30s | valid accuracy    0.743 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1506 batches | accuracy    0.701\n",
      "| epoch   8 |  1000/ 1506 batches | accuracy    0.730\n",
      "| epoch   8 |  1500/ 1506 batches | accuracy    0.714\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  7.21s | valid accuracy    0.743 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1506 batches | accuracy    0.711\n",
      "| epoch   9 |  1000/ 1506 batches | accuracy    0.713\n",
      "| epoch   9 |  1500/ 1506 batches | accuracy    0.720\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  7.20s | valid accuracy    0.743 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1506 batches | accuracy    0.719\n",
      "| epoch  10 |  1000/ 1506 batches | accuracy    0.724\n",
      "| epoch  10 |  1500/ 1506 batches | accuracy    0.702\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  7.27s | valid accuracy    0.743 \n",
      "-----------------------------------------------------------\n",
      "Test Accuracy: 0.753\n",
      "Test Precision: 0.776\n",
      "Test Recall: 0.753\n",
      "Test F1-Score: 0.664\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accuracy, precision, recall, f1 = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accuracy:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accuracy\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accuracy\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"text_classification_model.pth\")\n",
    "test_accuracy, test_precision, test_recall, test_f1 = evaluate(model, test_dataloader, criterion)\n",
    "print(\"Test Accuracy: {:.3f}\".format(test_accuracy))\n",
    "print(\"Test Precision: {:.3f}\".format(test_precision))\n",
    "print(\"Test Recall: {:.3f}\".format(test_recall))\n",
    "print(\"Test F1-Score: {:.3f}\".format(test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a not humor sentence.\n"
     ]
    }
   ],
   "source": [
    "sentiment_labels = ['not humor', 'humor']\n",
    "\n",
    "def predict(text, model, vocab, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # text = torch.tensor(vocab(basic_tokenizer(text)), device=device)\n",
    "        # text = torch.tensor(vocab(improved_tokenizer(text)), device=device)\n",
    "        text = torch.tensor(vocab(jieba_tokenizer(text)), device=device)\n",
    "\n",
    "        output = model(text, torch.tensor([0], device=device))\n",
    "        return labels[output.argmax(1).item()]\n",
    "\n",
    "ex_text_str = \"小刘说：我要卖油条\"\n",
    "print(\"This is a %s sentence.\" % (predict(ex_text_str, model, vocab, sentiment_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
