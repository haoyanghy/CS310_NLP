{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3. Recurrent Neural Networks for Language Modeling \n",
    "\n",
    "**Total points**: \n",
    "\n",
    "In this assignment, you will train a LSTM-based language model on the Harry Potter text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from gensim.downloader import load as gensim_load\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nltk_resources():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt_tab')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt_tab')\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=20000):\n",
    "    word_counts = Counter(tokens)\n",
    "    vocab = {word: idx + 2 for idx, (word, _) in \n",
    "             enumerate(word_counts.most_common(max_vocab_size - 2))}\n",
    "    # vocab = {word: idx + 2 for idx, (word, _) in enumerate(word_counts.items())}\n",
    "    vocab['<PAD>'] = 0  # Padding token\n",
    "    vocab['<UNK>'] = 1  # Unknown token for out-of-vocab words\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    return vocab, idx_to_word\n",
    "\n",
    "def tokens_to_tensor(tokens, vocab):\n",
    "    indices = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "class HarryPotterDataset(Dataset):\n",
    "    def __init__(self, tensor, sequence_length):\n",
    "        self.tensor = tensor\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tensor) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.tensor[idx:idx + self.sequence_length]\n",
    "        target_seq = self.tensor[idx + 1:idx + self.sequence_length + 1]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "def preprocess_and_load(file_path, sequence_length=20, batch_size=128, max_vocab_size=20000):\n",
    "    download_nltk_resources()\n",
    "    text = load_text(file_path)\n",
    "    tokens = tokenize_text(text)\n",
    "    vocab, idx_to_word = build_vocab(tokens, max_vocab_size)\n",
    "    tensor = tokens_to_tensor(tokens, vocab)\n",
    "    dataset = HarryPotterDataset(tensor, sequence_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataset, dataloader, vocab, idx_to_word, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n",
      "Total tokens: 1105952\n",
      "Sample batch shapes:\n",
      "Input: torch.Size([32, 20]), Target: torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Harry_Potter_all_books_preprocessed.txt\" \n",
    "sequence_length = 20 \n",
    "batch_size = 128\n",
    "max_vocab_size = 20000\n",
    "\n",
    "dataset, dataloader, vocab, idx_to_word, total_tokens = preprocess_and_load(\n",
    "    file_path, sequence_length, batch_size, max_vocab_size\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Total tokens: {total_tokens}\")\n",
    "print(f\"Sample batch shapes:\")\n",
    "for input_seq, target_seq in dataloader:\n",
    "    print(f\"Input: {input_seq.shape}, Target: {target_seq.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.2, embedding_matrix=None):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if embedding_matrix is None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "            \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)  \n",
    "        output, hidden = self.lstm(embedded, hidden) \n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        h0 = weight.new(self.num_layers, batch_size, self.hidden_dim).zero_()\n",
    "        c0 = weight.new(self.num_layers, batch_size, self.hidden_dim).zero_()\n",
    "        return (h0, c0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Output Shape: torch.Size([32, 20, 10000])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) \n",
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "num_epochs = 10 \n",
    "\n",
    "lstm_model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "\n",
    "# Dummy input (batch_size=128, sequence_length=20)\n",
    "batch_size, seq_len = 128, 20\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "hidden = lstm_model.init_hidden(batch_size)\n",
    "output, hidden = lstm_model(x, hidden)\n",
    "print(f\"LSTM Output Shape: {output.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, (input_seq, target_seq) in enumerate(train_loader):\n",
    "            if i % 1000 == 0: \n",
    "                    print(f\"Batch {i}/{len(train_loader)}\")\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            hidden = model.init_hidden(input_seq.size(0))\n",
    "            if isinstance(hidden, tuple):\n",
    "                hidden = tuple(h.to(device) for h in hidden)\n",
    "            else:\n",
    "                hidden = hidden.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_seq, target_seq in val_loader:\n",
    "                input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "                hidden = model.init_hidden(input_seq.size(0))\n",
    "                if isinstance(hidden, tuple):\n",
    "                    hidden = tuple(h.to(device) for h in hidden)\n",
    "                else:\n",
    "                    hidden = hidden.to(device)\n",
    "                output, hidden = model(input_seq, hidden)\n",
    "                val_loss += criterion(output.view(-1, vocab_size), target_seq.view(-1)).item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    return model, train_losses\n",
    "\n",
    "def compute_perplexity(model, test_loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.NLLLoss(ignore_index=0, reduction='none')\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in test_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            hidden = model.init_hidden(input_seq.size(0))\n",
    "            if isinstance(hidden, tuple):\n",
    "                hidden = tuple(h.to(device) for h in hidden)\n",
    "            else:\n",
    "                hidden = hidden.to(device)\n",
    "            \n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            log_probs = torch.log_softmax(output, dim=-1)\n",
    "            loss = loss_fn(log_probs.view(-1, vocab_size), target_seq.view(-1))\n",
    "            total_loss += loss.sum().item()\n",
    "            total_tokens += (target_seq != 0).sum().item()  # Count non-padding tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "# Greedy search generation\n",
    "def generate_sentence(model, start_tokens, vocab, idx_to_word, max_length=20, device='cuda'):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor([vocab.get(token, vocab['<UNK>']) for token in start_tokens], \n",
    "                           dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    if isinstance(hidden, tuple):\n",
    "        hidden = tuple(h.to(device) for h in hidden)\n",
    "    else:\n",
    "        hidden = hidden.to(device)\n",
    "    \n",
    "    generated = start_tokens.copy()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(start_tokens)):\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            next_token_idx = torch.argmax(output[:, -1, :], dim=-1).item()\n",
    "            generated.append(idx_to_word[next_token_idx])\n",
    "            input_seq = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
    "    \n",
    "    return ' '.join(generated)\n",
    "\n",
    "def load_glove_embeddings(vocab, embedding_dim=200):\n",
    "    glove = gensim_load('glove-wiki-gigaword-200')  # Download if not cached\n",
    "    embedding_matrix = torch.zeros(len(vocab), embedding_dim)\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove:\n",
    "            embedding_matrix[idx] = torch.tensor(glove[word])\n",
    "        else:\n",
    "            embedding_matrix[idx] = torch.randn(embedding_dim)  # Random for OOV\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Training LSTM...\n",
      "Batch 0/31105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining LSTM...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m lstm_model, lstm_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m glove_embeddings \u001b[38;5;241m=\u001b[39m load_glove_embeddings(vocab, embedding_dim)\n\u001b[0;32m     30\u001b[0m lstm_glove \u001b[38;5;241m=\u001b[39m LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout, glove_embeddings)\n",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m model(input_seq, hidden)\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), target_seq\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\aaron\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaron\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 90%-5%-5% split\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.9 * total_size)\n",
    "val_size = int(0.05 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "test_dataset = Subset(dataset, range(train_size + val_size, total_size))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "num_epochs = 10 \n",
    "\n",
    "lstm_model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "print(\"\\nTraining LSTM...\")\n",
    "lstm_model, lstm_losses = train_model(lstm_model, train_loader, val_loader, num_epochs, device)\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(vocab, embedding_dim)\n",
    "lstm_glove = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout, glove_embeddings)\n",
    "print(\"\\nTraining LSTM with GloVe embeddings...\")\n",
    "lstm_glove, glove_losses = train_model(lstm_glove, train_loader, val_loader, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perplexity\u001b[39;00m\n\u001b[0;32m      2\u001b[0m random_perplexity \u001b[38;5;241m=\u001b[39m compute_perplexity(lstm_model, test_loader, device)\n\u001b[1;32m----> 3\u001b[0m glove_perplexity \u001b[38;5;241m=\u001b[39m compute_perplexity(\u001b[43mlstm_glove\u001b[49m, test_loader, device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRandom Embeddings Test Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_perplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGloVe Embeddings Test Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglove_perplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm_glove' is not defined"
     ]
    }
   ],
   "source": [
    "# Perplexity\n",
    "random_perplexity = compute_perplexity(lstm_model, test_loader, device)\n",
    "glove_perplexity = compute_perplexity(lstm_glove, test_loader, device)\n",
    "print(f\"\\nRandom Embeddings Test Perplexity: {random_perplexity:.2f}\")\n",
    "print(f\"GloVe Embeddings Test Perplexity: {glove_perplexity:.2f}\")\n",
    "\n",
    "# Random Embeddings Test Perplexity: 414.89\n",
    "# GloVe Embeddings Test Perplexity: 500.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      "Prefix: harry looked\n",
      "LSTM: harry looked to the <UNK> of the <UNK> of the <UNK> of the <UNK> of the <UNK> of the <UNK>\n",
      "Prefix: the wand\n",
      "LSTM: the wand of the <UNK> of the <UNK> of the <UNK> of the <UNK> of the <UNK> of the <UNK>\n",
      "Prefix: hermione said\n",
      "LSTM: hermione said harry was the <UNK> of the <UNK> of the <UNK> of the <UNK> of the <UNK> of the\n",
      "Prefix: ron grabbed\n",
      "LSTM: ron grabbed the <UNK> of the <UNK> of the <UNK> of the <UNK> of the <UNK> of the <UNK> of\n",
      "Prefix: dumbledore smiled\n",
      "LSTM: dumbledore smiled the <UNK> of the <UNK> of the <UNK> of the <UNK> of the <UNK> of the <UNK> of\n"
     ]
    }
   ],
   "source": [
    "# Generate sentences\n",
    "prefixes = [\n",
    "    [\"harry\", \"looked\"],\n",
    "    [\"the\", \"wand\"],\n",
    "    [\"hermione\", \"said\"],\n",
    "    [\"ron\", \"grabbed\"],\n",
    "    [\"dumbledore\", \"smiled\"]\n",
    "]\n",
    "\n",
    "print(\"\\nGenerated Sentences:\")\n",
    "for prefix in prefixes:\n",
    "    lstm_sentence = generate_sentence(lstm_model, prefix, vocab, idx_to_word, device=device)\n",
    "    print(f\"Prefix: {' '.join(prefix)}\")\n",
    "    print(f\"LSTM: {lstm_sentence}\")\n",
    "    \n",
    "'''\n",
    "Generated Sentences:\n",
    "Prefix: harry looked\n",
    "LSTM: harry looked around at the door of the field and the crowd were still in the middle of the field\n",
    "Prefix: the wand\n",
    "LSTM: the wand and the rest of the team were sitting in the middle of the field and the crowd below\n",
    "Prefix: hermione said\n",
    "LSTM: hermione said quietly .i dont think you could have been able to get rid of the bandon banshee and is\n",
    "Prefix: ron grabbed\n",
    "LSTM: ron grabbed the table and pulled out his wand and began to bleed afresh .i was not sure that he\n",
    "Prefix: dumbledore smiled\n",
    "LSTM: dumbledore smiled at him as though he had been bidden to memorize it as though he had been bidden to\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot training loss curves\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[43mlstm_losses\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), glove_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGloVe Embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), lstm_losses, label=\"Random Embeddings\")\n",
    "plt.plot(range(1, num_epochs + 1), glove_losses, label=\"GloVe Embeddings\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss Curves: Random vs. GloVe Embeddings\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"loss_curves.png\") \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
