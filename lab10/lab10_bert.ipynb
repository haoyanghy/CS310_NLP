{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 10: Explore BERT\n",
    "\n",
    "In this lab, we will practice using pre-trained BERT models provided by the HuggingFace `transformers` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Explore Pretrained BERT Model\n",
    "\n",
    "In this task, you will explore the pretrained BERT model using the Hugging Face Transformers library. \n",
    "\n",
    "First, you will load a pretrained BERT model and the correponding tokenizer. If you use the default model string `'bert-base-uncased'`, it will automatically download the model.\n",
    "\n",
    "In our case, to avoid any network issue, you can follow these steps to load the model locally:\n",
    "- Download the `bert-base-uncased.zip` file from the course website and unzip it to the folder `bert-base-uncased` in the same directory as this notebook. \n",
    "- When you load the model, you simply specify the folder path `bert-base-uncased/` (which contains all model files) to the `from_pretrained()` function. \n",
    "- *Note* that don't exclude the last `/` in the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased/') # Make sure you download the model files first\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by counting the number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tensors:  199\n"
     ]
    }
   ],
   "source": [
    "n_tensors = 0\n",
    "for param in bert_model.parameters():\n",
    "    n_tensors += 1\n",
    "\n",
    "print(\"Number of tensors: \", n_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  109482240\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "for param in bert_model.parameters():\n",
    "    n_params += param.numel()\n",
    "\n",
    "print(\"Number of parameters: \", n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if you are interested in how the parameters are organized, you can print the model's `_modules` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embeddings', BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")), ('encoder', BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSdpaSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")), ('pooler', BertPooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "))])\n"
     ]
    }
   ],
   "source": [
    "print(bert_model._modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, you can access the parameters at any layer of the model, by specifying the layer name and index. \n",
    "\n",
    "For example, if you want to check the the query matrix $W^Q$ in the self-attention layer of the first transformer block, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True)\n"
     ]
    }
   ],
   "source": [
    "pprint(bert_model._modules['encoder']._modules['layer'][0]._modules['attention']._modules['self']._modules['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the $W^Q$ matrix is implemented as a `nn.Linear` module.\n",
    "\n",
    "Also, the same inquiry can be simplified by using the `get_submodule()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "W_q = bert_model.get_submodule('encoder.layer.0.attention.self.query')\n",
    "print(W_q)\n",
    "print(W_q.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Get Contextual Embeddings from BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on and use the BERT model to get contextual embeddings for given texts.\n",
    "\n",
    "First, we prepare some sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have a new CPU!',\n",
      " 'I have a new Intel CPU!',\n",
      " 'I have a new GPU!',\n",
      " 'I have a new NVIDIA GPU!']\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "        'I have a new CPU!\\n'\n",
    "        'I have a new Intel CPU!\\n'\n",
    "        'I have a new GPU!\\n'\n",
    "        'I have a new NVIDIA GPU!'\n",
    "    )\n",
    "\n",
    "sentences = text.split('\\n')\n",
    "pprint(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the `tokenize()` function of the previously initialized BERT tokenizer on each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'have', 'a', 'new', 'cpu', '!'],\n",
      " ['i', 'have', 'a', 'new', 'intel', 'cpu', '!'],\n",
      " ['i', 'have', 'a', 'new', 'gp', '##u', '!'],\n",
      " ['i', 'have', 'a', 'new', 'n', '##vid', '##ia', 'gp', '##u', '!']]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "tokens_in_string = [bert_tokenizer.tokenize(s) for s in sentences]\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "pprint(tokens_in_string)\n",
    "# You should expect to see the following output:\n",
    "# [['i', 'have', 'a', 'new', 'cpu', '!'],\n",
    "#  ['i', 'have', 'a', 'new', 'intel', 'cpu', '!'],\n",
    "#  ['i', 'have', 'a', 'new', 'gp', '##u', '!'],\n",
    "#  ['i', 'have', 'a', 'new', 'n', '##vid', '##ia', 'gp', '##u', '!']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that \"CPU\" and \"Intel\" are recognized as whole words, but \"NVIDIA\" and \"GPU\" are not. Thus, they appear as subwords such as \"##u\" \"##vid\" in the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are not integer token IDs yet, so now use the `batch_encode()` function, with argument `return_tensors='pt'`, to convert each sentence to integer token IDs. Here `'pt'` is for PyTorch tensors.\n",
    "\n",
    "**Note**:\n",
    "- Each token is represented as an integer in `torch.int64` data type.\n",
    "- By default, the tokenizer adds special tokens `[CLS]` and `[SEP]` to the beginning and end of each sentence, which correpond to the token ID `101` and `102`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "[tensor([[  101,  1045,  2031,  1037,  2047, 17368,   999,   102]]),\n",
      " tensor([[  101,  1045,  2031,  1037,  2047, 13420, 17368,   999,   102]]),\n",
      " tensor([[  101,  1045,  2031,  1037,  2047, 14246,  2226,   999,   102]]),\n",
      " tensor([[  101,  1045,  2031,  1037,  2047,  1050, 17258,  2401, 14246,  2226,\n",
      "           999,   102]])]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "token_ids_list = [bert_tokenizer.encode(tokens, return_tensors='pt') for tokens in tokens_in_string]\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print(token_ids_list[0].dtype)\n",
    "pprint(token_ids_list)\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# torch.int64\n",
    "# [tensor([[  101,  1045,  2031,  1037,  2047, 17368,   999,   102]]),\n",
    "#  tensor([[  101,  1045,  2031,  1037,  2047, 13420, 17368,   999,   102]]),\n",
    "#  tensor([[  101,  1045,  2031,  1037,  2047, 14246,  2226,   999,   102]]),\n",
    "#  tensor([[  101,  1045,  2031,  1037,  2047,  1050, 17258,  2401, 14246,  2226,\n",
    "#            999,   102]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now `\"CPU\"` is tokenized to `17368`, `\"Intel\"` to `13420`, while `\"GPU\"` to `[14246, 2226]`, and `\"NVIDIA\"` to `[1050, 17258,  2401]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `ids_to_tokens` dictionary to map integer token IDs back to token strings, and use `decode()` function to convert a list of token IDs back to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "[SEP]\n",
      "cpu\n",
      "[CLS] i have a new cpu! [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.ids_to_tokens[101])\n",
    "print(bert_tokenizer.ids_to_tokens[102])\n",
    "print(bert_tokenizer.ids_to_tokens[17368])\n",
    "print(bert_tokenizer.decode(token_ids_list[0].squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in last example above, we `squeeze` the token IDs first, becaseu the encoded IDs are of dimension $1\\times N$, where $N$ is sentence length, because PyTorch uses first dimension as batch size.\n",
    "\n",
    "It indicates that we can tokenize multiple sentences in one batch by using the `batch_encode_plus()` function, and specify the argument `padding=True` to pad all sentences to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2031,  1037,  2047, 17368,   999,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2031,  1037,  2047, 13420, 17368,   999,   102,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2031,  1037,  2047, 14246,  2226,   999,   102,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2031,  1037,  2047,  1050, 17258,  2401, 14246,  2226,\n",
      "           999,   102]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = bert_tokenizer.batch_encode_plus(sentences, return_tensors='pt', padding=True, return_attention_mask=False, return_token_type_ids=False)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the returned dictionary contains an item keyed by `'input_ids'`, which is exactly the token IDs we need. \n",
    "\n",
    "**Note**:\n",
    "- It is a tensor of shape $B\\times N$, where $B$ is the batch size (here, $B=4$) and $N$ is the maximum sentence length in the batch.\n",
    "- The default padding token is `0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we deliberately set `return_attention_mask=False` to show simpler results. \n",
    "\n",
    "If you set it to `True`, then then returned dictionary will also contain an item keyed by `'attention_mask'`, which is a tensor of shape $B\\times N$ with `1` for real tokens and `0` for padding tokens. This information is useful for follow-up computations.\n",
    "\n",
    "Try if you can get the attention mask tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "encoded_sentences = bert_tokenizer.batch_encode_plus(sentences, return_tensors='pt', padding=True, return_attention_mask=True, return_token_type_ids=False)\n",
    "attn_mask = encoded_sentences['attention_mask']\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(attn_mask)\n",
    "# You should expect to see the following output:\n",
    "# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's obtain the contextual embeddings for the four target words `\"CPU\"`, `\"Intel\"`, `\"NVIDIA\"`, and `\"GPU\"` in our sentences.\n",
    "\n",
    "First, pass the token IDs in one batch to the BERT model to get the output object, which has a `last_hidden_state` attribute that contains the contextual embeddings.\n",
    "\n",
    "**Note**:\n",
    "- You can manually specify `input_ids` and `attention_mask` as the input arguments to the model.\n",
    "- Or you can directly pass the dictionary returned by `batch_encode_plus()` to the model, and use the `**` operator as most tutorials did:\n",
    "  - `outputs = model(**encoded_sentences)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### START YOUR CODE ###\n",
    "    outputs = bert_model(**encoded_sentences)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print(outputs.last_hidden_state.shape)\n",
    "# You should expect to see the following output:\n",
    "# torch.Size([4, 12, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for ``\"CPU\"`` and ``\"Intel\"``, you can directly use the output vectors at the corresponding positions, because they are recognized as whole words.\n",
    "\n",
    "Compute the average vector of `\"CPU\"`s in the first two sentences, and compute its cosine similarity with the vector of `\"Intel\"`.\n",
    "\n",
    "*Hint*:\n",
    "- Use `F.cosine_similarity()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_cpu_intel: 0.7551644444465637\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "vec_cpu1 = outputs.last_hidden_state[0, 5, :] # bert tokenizer adds [CLS] token at the beginning\n",
    "vec_cpu2 = outputs.last_hidden_state[1, 6, :]\n",
    "vec_cpu_avg = (vec_cpu1 + vec_cpu2) / 2\n",
    "vec_intel = outputs.last_hidden_state[1, 5, :]\n",
    "cos_cpu_intel = F.cosine_similarity(vec_cpu_avg, vec_intel, dim=-1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_cpu_intel:', cos_cpu_intel.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_cpu_intel: 0.7551645636558533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `\"NVIDIA\"` and `\"GPU\"`, it's a bit trickier, as you need to use the sum of subword vectors to get the vector of the whole word.\n",
    "\n",
    "In sentence 3, `\"GPU\"` is tokenized to `[14246, 2226]`, so you need to sum the vectors at these two positions.\n",
    "\n",
    "In sentence 4, `\"NVIDIA\"` is tokenized to `[1050, 17258,  2401]`, so you need to sum the vectors at these three positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_gpu_nv: 0.7273839116096497\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "vec_gpu1 = outputs.last_hidden_state[2, 5:7, :].sum(dim=0)\n",
    "vec_gpu2 = outputs.last_hidden_state[3, 8:10, :].sum(dim=0)\n",
    "vec_gpu = vec_gpu1 + vec_gpu2\n",
    "vec_nvidia = outputs.last_hidden_state[3, 5:8, :].sum(dim=0)\n",
    "cos_gpu_nv = F.cosine_similarity(vec_gpu, vec_nvidia, dim=0)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_gpu_nv:', cos_gpu_nv.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_gpu_nv: 0.7273837327957153"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if `\"NVIDIA\"` is closer to `\"GPU\"` than `\"CPU\"`, and vice versa for `\"Intel\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_cpu_nv: 0.5931226015090942\n",
      "cos_gpu_intel: 0.5778647661209106\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "cos_cpu_nv = F.cosine_similarity(vec_cpu_avg, vec_nvidia, dim=-1)\n",
    "cos_gpu_intel = F.cosine_similarity(vec_gpu, vec_intel, dim=-1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_cpu_nv:', cos_cpu_nv.item())\n",
    "print('cos_gpu_intel:', cos_gpu_intel.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_cpu_nv: 0.5931224226951599\n",
    "# cos_gpu_intel: 0.5778647661209106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting, right?\n",
    "\n",
    "How about the distance between the two products `\"CPU\"` and `\"GPU\"`? or between the two companies `\"Intel\"` and `\"NVIDIA\"`? Check it out yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_cpu_gpu: 0.6914966106414795\n",
      "cos_intel_nv: 0.6179742813110352\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "cos_cpu_gpu = F.cosine_similarity(vec_cpu_avg, vec_gpu, dim=-1)\n",
    "cos_intel_nv = F.cosine_similarity(vec_intel, vec_nvidia, dim=-1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_cpu_gpu:', cos_cpu_gpu.item())\n",
    "print('cos_intel_nv:', cos_intel_nv.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_cpu_gpu: 0.6914964914321899\n",
    "# cos_intel_nv: 0.6179742813110352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Access all hidden states\n",
    "\n",
    "Let's be more adventurous and access all hidden states returned by the BERT model.\n",
    "\n",
    "*Hint*: Simply set the argument `output_hidden_states=True` when calling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "13\n",
      "torch.Size([4, 12, 768])\n",
      "torch.Size([4, 12, 768])\n",
      "torch.Size([4, 12, 768])\n",
      "torch.Size([4, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### START YOUR CODE ###\n",
    "    outputs = bert_model(**encoded_sentences, output_hidden_states=True)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(type(outputs.hidden_states))\n",
    "print(len(outputs.hidden_states))\n",
    "print(outputs.hidden_states[-1].shape)\n",
    "print(outputs.hidden_states[-2].shape)\n",
    "print(outputs.hidden_states[-3].shape)\n",
    "print(outputs.hidden_states[-4].shape)\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# <class 'tuple'>\n",
    "# 13\n",
    "# torch.Size([4, 12, 768])\n",
    "# torch.Size([4, 12, 768])\n",
    "# torch.Size([4, 12, 768])\n",
    "# torch.Size([4, 12, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average vector of the word `\"CPU\"` in the first sentence, using the hidden states of the last **four** layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos: 0.900215208530426\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "vec_last4 = outputs.hidden_states[-4:]\n",
    "cpu_last4 = [tensor[0,5,:] for tensor in vec_last4]\n",
    "vec_cpu_avg_last4 = torch.stack(cpu_last4).mean(dim=0)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "cos = F.cosine_similarity(vec_cpu_avg_last4, vec_cpu_avg, dim=0)\n",
    "print('cos:', cos.item())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# cos: 0.9002149701118469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Fine tune a BERT model for text classification task\n",
    "\n",
    "For the last task, we will practice fine-tuning a BERT-based model for a text classification task -- sentiment analysis on IMDB movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "imdb = load_dataset('imdb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two fields in this dataset:\n",
    "- `text`: a string, the review text\n",
    "- `label`: an integer, 0 for negative, 1 for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV '\n",
      "         'are usually underfunded, under-appreciated and misunderstood. I '\n",
      "         'tried to like this, I really did, but it is to good TV sci-fi as '\n",
      "         'Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap '\n",
      "         \"cardboard sets, stilted dialogues, CG that doesn't match the \"\n",
      "         'background, and painfully one-dimensional characters cannot be '\n",
      "         \"overcome with a 'sci-fi' setting. (I'm sure there are those of you \"\n",
      "         \"out there who think Babylon 5 is good sci-fi TV. It's not. It's \"\n",
      "         'clichéd and uninspiring.) While US viewers might like emotion and '\n",
      "         'character development, sci-fi is a genre that does not take itself '\n",
      "         'seriously (cf. Star Trek). It may treat important issues, yet not as '\n",
      "         \"a serious philosophy. It's really difficult to care about the \"\n",
      "         'characters here as they are not simply foolish, just missing a spark '\n",
      "         'of life. Their actions and reactions are wooden and predictable, '\n",
      "         \"often painful to watch. The makers of Earth KNOW it's rubbish as \"\n",
      "         'they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise '\n",
      "         \"people would not continue watching. Roddenberry's ashes must be \"\n",
      "         'turning in their orbit as this dull, cheap, poorly edited (watching '\n",
      "         'it without advert breaks really brings this home) trudging Trabant '\n",
      "         'of a show lumbers into space. Spoiler. So, kill off a main '\n",
      "         'character. And then bring him back as another actor. Jeeez! Dallas '\n",
      "         'all over again.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(imdb['test'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a DistilBERT model tokenizer to process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preprocessing function to tokenize the `text` field of an example with truncation, so that it does not exceed the maximum length of the model (512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.map()` function to apply the preocessing function to the entire dataset, and speed it up using `batched=True`\n",
    "\n",
    "(takes a few seconds to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7202335a764d86903e7da05aa81624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_imdb = imdb.map(preprocess_imdb, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all `text` field are tokenized to `input_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are '\n",
      " 'usually underfunded, under-appreciated and misunderstood. I tried to like '\n",
      " 'this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek '\n",
      " '(the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, '\n",
      " \"CG that doesn't match the background, and painfully one-dimensional \"\n",
      " \"characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are \"\n",
      " \"those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's \"\n",
      " 'clichéd and uninspiring.) While US viewers might like emotion and character '\n",
      " 'development, sci-fi is a genre that does not take itself seriously (cf. Star '\n",
      " \"Trek). It may treat important issues, yet not as a serious philosophy. It's \"\n",
      " 'really difficult to care about the characters here as they are not simply '\n",
      " 'foolish, just missing a spark of life. Their actions and reactions are '\n",
      " 'wooden and predictable, often painful to watch. The makers of Earth KNOW '\n",
      " 'it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" '\n",
      " \"otherwise people would not continue watching. Roddenberry's ashes must be \"\n",
      " 'turning in their orbit as this dull, cheap, poorly edited (watching it '\n",
      " 'without advert breaks really brings this home) trudging Trabant of a show '\n",
      " 'lumbers into space. Spoiler. So, kill off a main character. And then bring '\n",
      " 'him back as another actor. Jeeez! Dallas all over again.')\n",
      "[101,\n",
      " 1045,\n",
      " 2293,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 1998,\n",
      " 2572,\n",
      " 5627,\n",
      " 2000,\n",
      " 2404,\n",
      " 2039,\n",
      " 2007,\n",
      " 1037,\n",
      " 2843,\n",
      " 1012,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 5691,\n",
      " 1013,\n",
      " 2694,\n",
      " 2024,\n",
      " 2788,\n",
      " 2104,\n",
      " 11263,\n",
      " 25848,\n",
      " 1010,\n",
      " 2104,\n",
      " 1011,\n",
      " 12315,\n",
      " 1998,\n",
      " 28947,\n",
      " 1012,\n",
      " 1045,\n",
      " 2699,\n",
      " 2000,\n",
      " 2066,\n",
      " 2023,\n",
      " 1010,\n",
      " 1045,\n",
      " 2428,\n",
      " 2106,\n",
      " 1010,\n",
      " 2021,\n",
      " 2009,\n",
      " 2003,\n",
      " 2000,\n",
      " 2204,\n",
      " 2694,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 2004,\n",
      " 17690,\n",
      " 1019,\n",
      " 2003,\n",
      " 2000,\n",
      " 2732,\n",
      " 10313,\n",
      " 1006,\n",
      " 1996,\n",
      " 2434,\n",
      " 1007,\n",
      " 1012,\n",
      " 10021,\n",
      " 4013,\n",
      " 3367,\n",
      " 20086,\n",
      " 2015,\n",
      " 1010,\n",
      " 10036,\n",
      " 19747,\n",
      " 4520,\n",
      " 1010,\n",
      " 25931,\n",
      " 3064,\n",
      " 22580,\n",
      " 1010,\n",
      " 1039,\n",
      " 2290,\n",
      " 2008,\n",
      " 2987,\n",
      " 1005,\n",
      " 1056,\n",
      " 2674,\n",
      " 1996,\n",
      " 4281,\n",
      " 1010,\n",
      " 1998,\n",
      " 16267,\n",
      " 2028,\n",
      " 1011,\n",
      " 8789,\n",
      " 3494,\n",
      " 3685,\n",
      " 2022,\n",
      " 9462,\n",
      " 2007,\n",
      " 1037,\n",
      " 1005,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 1005,\n",
      " 4292,\n",
      " 1012,\n",
      " 1006,\n",
      " 1045,\n",
      " 1005,\n",
      " 1049,\n",
      " 2469,\n",
      " 2045,\n",
      " 2024,\n",
      " 2216,\n",
      " 1997,\n",
      " 2017,\n",
      " 2041,\n",
      " 2045,\n",
      " 2040,\n",
      " 2228,\n",
      " 17690,\n",
      " 1019,\n",
      " 2003,\n",
      " 2204,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 2694,\n",
      " 1012,\n",
      " 2009,\n",
      " 1005,\n",
      " 1055,\n",
      " 2025,\n",
      " 1012,\n",
      " 2009,\n",
      " 1005,\n",
      " 1055,\n",
      " 18856,\n",
      " 17322,\n",
      " 2094,\n",
      " 1998,\n",
      " 4895,\n",
      " 7076,\n",
      " 8197,\n",
      " 4892,\n",
      " 1012,\n",
      " 1007,\n",
      " 2096,\n",
      " 2149,\n",
      " 7193,\n",
      " 2453,\n",
      " 2066,\n",
      " 7603,\n",
      " 1998,\n",
      " 2839,\n",
      " 2458,\n",
      " 1010,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 2003,\n",
      " 1037,\n",
      " 6907,\n",
      " 2008,\n",
      " 2515,\n",
      " 2025,\n",
      " 2202,\n",
      " 2993,\n",
      " 5667,\n",
      " 1006,\n",
      " 12935,\n",
      " 1012,\n",
      " 2732,\n",
      " 10313,\n",
      " 1007,\n",
      " 1012,\n",
      " 2009,\n",
      " 2089,\n",
      " 7438,\n",
      " 2590,\n",
      " 3314,\n",
      " 1010,\n",
      " 2664,\n",
      " 2025,\n",
      " 2004,\n",
      " 1037,\n",
      " 3809,\n",
      " 4695,\n",
      " 1012,\n",
      " 2009,\n",
      " 1005,\n",
      " 1055,\n",
      " 2428,\n",
      " 3697,\n",
      " 2000,\n",
      " 2729,\n",
      " 2055,\n",
      " 1996,\n",
      " 3494,\n",
      " 2182,\n",
      " 2004,\n",
      " 2027,\n",
      " 2024,\n",
      " 2025,\n",
      " 3432,\n",
      " 13219,\n",
      " 1010,\n",
      " 2074,\n",
      " 4394,\n",
      " 1037,\n",
      " 12125,\n",
      " 1997,\n",
      " 2166,\n",
      " 1012,\n",
      " 2037,\n",
      " 4506,\n",
      " 1998,\n",
      " 9597,\n",
      " 2024,\n",
      " 4799,\n",
      " 1998,\n",
      " 21425,\n",
      " 1010,\n",
      " 2411,\n",
      " 9145,\n",
      " 2000,\n",
      " 3422,\n",
      " 1012,\n",
      " 1996,\n",
      " 11153,\n",
      " 1997,\n",
      " 3011,\n",
      " 2113,\n",
      " 2009,\n",
      " 1005,\n",
      " 1055,\n",
      " 29132,\n",
      " 2004,\n",
      " 2027,\n",
      " 2031,\n",
      " 2000,\n",
      " 2467,\n",
      " 2360,\n",
      " 1000,\n",
      " 4962,\n",
      " 8473,\n",
      " 4181,\n",
      " 9766,\n",
      " 1005,\n",
      " 1055,\n",
      " 3011,\n",
      " 1012,\n",
      " 1012,\n",
      " 1012,\n",
      " 1000,\n",
      " 4728,\n",
      " 2111,\n",
      " 2052,\n",
      " 2025,\n",
      " 3613,\n",
      " 3666,\n",
      " 1012,\n",
      " 8473,\n",
      " 4181,\n",
      " 9766,\n",
      " 1005,\n",
      " 1055,\n",
      " 11289,\n",
      " 2442,\n",
      " 2022,\n",
      " 3810,\n",
      " 1999,\n",
      " 2037,\n",
      " 8753,\n",
      " 2004,\n",
      " 2023,\n",
      " 10634,\n",
      " 1010,\n",
      " 10036,\n",
      " 1010,\n",
      " 9996,\n",
      " 5493,\n",
      " 1006,\n",
      " 3666,\n",
      " 2009,\n",
      " 2302,\n",
      " 4748,\n",
      " 16874,\n",
      " 7807,\n",
      " 2428,\n",
      " 7545,\n",
      " 2023,\n",
      " 2188,\n",
      " 1007,\n",
      " 19817,\n",
      " 6784,\n",
      " 4726,\n",
      " 19817,\n",
      " 19736,\n",
      " 3372,\n",
      " 1997,\n",
      " 1037,\n",
      " 2265,\n",
      " 13891,\n",
      " 2015,\n",
      " 2046,\n",
      " 2686,\n",
      " 1012,\n",
      " 27594,\n",
      " 2121,\n",
      " 1012,\n",
      " 2061,\n",
      " 1010,\n",
      " 3102,\n",
      " 2125,\n",
      " 1037,\n",
      " 2364,\n",
      " 2839,\n",
      " 1012,\n",
      " 1998,\n",
      " 2059,\n",
      " 3288,\n",
      " 2032,\n",
      " 2067,\n",
      " 2004,\n",
      " 2178,\n",
      " 3364,\n",
      " 1012,\n",
      " 15333,\n",
      " 4402,\n",
      " 2480,\n",
      " 999,\n",
      " 5759,\n",
      " 2035,\n",
      " 2058,\n",
      " 2153,\n",
      " 1012,\n",
      " 102]\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenized_imdb['test'][0]['text'])\n",
    "pprint(tokenized_imdb['test'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `DataCollatorWithPadding` to pad the sequences in one batch to the longest sequence in the batch *dynamically*. \n",
    "\n",
    "This is a more efficient way than padding in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define a DistilBERT model as an instance of `AutoModelForSequenceClassification` with 2 output classes (positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased/\", num_labels=2)\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works on one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1252, 0.0530]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_tensor = torch.tensor(tokenized_imdb['test'][0]['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(tokenized_imdb['test'][0]['attention_mask']).unsqueeze(0).to(device)\n",
    "    outputs = model(input_ids=input_tensor, attention_mask=attention_mask)\n",
    "    print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the `TrainingArguments` and `Trainer` from the `transformers` library to fine tune the model.\n",
    "\n",
    "- Training hyperparameters are set in `TrainingArguments`\n",
    "- `Trainer` takes model, tokenizer, dataset, data_collator, and training arguments as input\n",
    "- Call `trainer.train()` to start finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaron\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    fp16=True if torch.cuda.is_available() else False,  # Mixed precision\n",
    "    gradient_accumulation_steps=8, \n",
    "    report_to=\"none\",  # Disable logging to external services\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching the trainer, we will need an evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "# If you have problem connecting to huggingface, you can git clone the evaluate repo https://github.com/huggingface/evaluate.git\n",
    "# and copy the `metrics/accuracy` folder to your current directory\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"].select(range(5000)),  # Use a smaller subset for faster training\n",
    "    eval_dataset=tokenized_imdb[\"test\"].select(range(5000)),  # Use a smaller subset for faster evaluation\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are good to go!\n",
    "\n",
    "Note that it runs very slowly on CPU, and you better wrap up all the code to one Python script and run it on a GPU server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b64109805ef474d828e3f181bc09879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f644eb1657f94f99ae74b44fa81ad7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0011771568097174168, 'eval_accuracy': 1.0, 'eval_runtime': 646.4633, 'eval_samples_per_second': 7.734, 'eval_steps_per_second': 1.934, 'epoch': 1.0}\n",
      "{'train_runtime': 3380.5811, 'train_samples_per_second': 1.479, 'train_steps_per_second': 0.046, 'train_loss': 0.02827310256468944, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('output/final_model\\\\tokenizer_config.json',\n",
       " 'output/final_model\\\\special_tokens_map.json',\n",
       " 'output/final_model\\\\vocab.txt',\n",
       " 'output/final_model\\\\added_tokens.json',\n",
       " 'output/final_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"output/final_model\")\n",
    "tokenizer.save_pretrained(\"output/final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
